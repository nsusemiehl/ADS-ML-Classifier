{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Doc2Vec Model\n",
    "\n",
    "Introduces Gensim's Doc2Vec model and demonstrates its use on the\n",
    "[Lee Corpus](https://hekyll.services.adelaide.edu.au/dspace/bitstream/2440/28910/1/hdl_28910.pdf)_.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doc2Vec is a `core_concepts_model` that represents each\n",
    "`core_concepts_document` as a `core_concepts_vector`.  This\n",
    "tutorial introduces the model and demonstrates how to train and assess it.\n",
    "\n",
    "Here's a list of what we'll be doing:\n",
    "\n",
    "0. Review the relevant models: bag-of-words, Word2Vec, Doc2Vec\n",
    "1. Load and preprocess the training and test corpora (see `core_concepts_corpus`)\n",
    "2. Train a Doc2Vec `core_concepts_model` model using the training corpus\n",
    "3. Demonstrate how the trained model can be used to infer a `core_concepts_vector`\n",
    "4. Assess the model\n",
    "5. Test the model on the test corpus\n",
    "\n",
    "## Review: Bag-of-words\n",
    "\n",
    ".. Note:: Feel free to skip these review sections if you're already familiar with the models.\n",
    "\n",
    "You may be familiar with the [bag-of-words model](https://en.wikipedia.org/wiki/Bag-of-words_model) from the\n",
    "`core_concepts_vector` section.\n",
    "This model transforms each document to a fixed-length vector of integers.\n",
    "For example, given the sentences:\n",
    "\n",
    "- ``John likes to watch movies. Mary likes movies too.``\n",
    "- ``John also likes to watch football games. Mary hates football.``\n",
    "\n",
    "The model outputs the vectors:\n",
    "\n",
    "- ``[1, 2, 1, 1, 2, 1, 1, 0, 0, 0, 0]``\n",
    "- ``[1, 1, 1, 1, 0, 1, 0, 1, 2, 1, 1]``\n",
    "\n",
    "Each vector has 10 elements, where each element counts the number of times a\n",
    "particular word occurred in the document.\n",
    "The order of elements is arbitrary.\n",
    "In the example above, the order of the elements corresponds to the words:\n",
    "``[\"John\", \"likes\", \"to\", \"watch\", \"movies\", \"Mary\", \"too\", \"also\", \"football\", \"games\", \"hates\"]``.\n",
    "\n",
    "Bag-of-words models are surprisingly effective, but have several weaknesses.\n",
    "\n",
    "First, they lose all information about word order: \"John likes Mary\" and\n",
    "\"Mary likes John\" correspond to identical vectors. There is a solution: bag\n",
    "of [n-grams](https://en.wikipedia.org/wiki/N-gram)_\n",
    "models consider word phrases of length n to represent documents as\n",
    "fixed-length vectors to capture local word order but suffer from data\n",
    "sparsity and high dimensionality.\n",
    "\n",
    "Second, the model does not attempt to learn the meaning of the underlying\n",
    "words, and as a consequence, the distance between vectors doesn't always\n",
    "reflect the difference in meaning.  The ``Word2Vec`` model addresses this\n",
    "second problem.\n",
    "\n",
    "## Review: ``Word2Vec`` Model\n",
    "\n",
    "``Word2Vec`` is a more recent model that embeds words in a lower-dimensional\n",
    "vector space using a shallow neural network. The result is a set of\n",
    "word-vectors where vectors close together in vector space have similar\n",
    "meanings based on context, and word-vectors distant to each other have\n",
    "differing meanings. For example, ``strong`` and ``powerful`` would be close\n",
    "together and ``strong`` and ``Paris`` would be relatively far.\n",
    "\n",
    "Gensim's :py:class:`~gensim.models.word2vec.Word2Vec` class implements this model.\n",
    "\n",
    "With the ``Word2Vec`` model, we can calculate the vectors for each **word** in a document.\n",
    "But what if we want to calculate a vector for the **entire document**\\ ?\n",
    "We could average the vectors for each word in the document - while this is quick and crude, it can often be useful.\n",
    "However, there is a better way...\n",
    "\n",
    "## Introducing: Paragraph Vector\n",
    "\n",
    ".. Important:: In Gensim, we refer to the Paragraph Vector model as ``Doc2Vec``.\n",
    "\n",
    "Le and Mikolov in 2014 introduced the [Doc2Vec algorithm](https://cs.stanford.edu/~quocle/paragraph_vector.pdf)_,\n",
    "which usually outperforms such simple-averaging of ``Word2Vec`` vectors.\n",
    "\n",
    "The basic idea is: act as if a document has another floating word-like\n",
    "vector, which contributes to all training predictions, and is updated like\n",
    "other word-vectors, but we will call it a doc-vector. Gensim's\n",
    ":py:class:`~gensim.models.doc2vec.Doc2Vec` class implements this algorithm.\n",
    "\n",
    "There are two implementations:\n",
    "\n",
    "1. Paragraph Vector - Distributed Memory (PV-DM)\n",
    "2. Paragraph Vector - Distributed Bag of Words (PV-DBOW)\n",
    "\n",
    ".. Important::\n",
    "  Don't let the implementation details below scare you.\n",
    "  They're advanced material: if it's too much, then move on to the next section.\n",
    "\n",
    "PV-DM is analogous to Word2Vec CBOW. The doc-vectors are obtained by training\n",
    "a neural network on the synthetic task of predicting a center word based an\n",
    "average of both context word-vectors and the full document's doc-vector.\n",
    "\n",
    "PV-DBOW is analogous to Word2Vec SG. The doc-vectors are obtained by training\n",
    "a neural network on the synthetic task of predicting a target word just from\n",
    "the full document's doc-vector. (It is also common to combine this with\n",
    "skip-gram testing, using both the doc-vector and nearby word-vectors to\n",
    "predict a single target word, but only one at a time.)\n",
    "\n",
    "## Prepare the Training and Test Data\n",
    "\n",
    "For this tutorial, we'll be training our model using the [Lee Background\n",
    "Corpus](https://hekyll.services.adelaide.edu.au/dspace/bitstream/2440/28910/1/hdl_28910.pdf)\n",
    "included in gensim. This corpus contains 314 documents selected from the\n",
    "Australian Broadcasting Corporationâ€™s news mail service, which provides text\n",
    "e-mails of headline stories and covers a number of broad topics.\n",
    "\n",
    "And we'll test our model by eye using the much shorter [Lee Corpus](https://hekyll.services.adelaide.edu.au/dspace/bitstream/2440/28910/1/hdl_28910.pdf)\n",
    "which contains 50 documents.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gensim\n",
    "# Set file names for train and test data\n",
    "test_data_dir = os.path.join(gensim.__path__[0], 'test', 'test_data')\n",
    "lee_train_file = os.path.join(test_data_dir, 'lee_background.cor')\n",
    "lee_test_file = os.path.join(test_data_dir, 'lee.cor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a Function to Read and Preprocess Text\n",
    "\n",
    "Below, we define a function to:\n",
    "\n",
    "- open the train/test file (with latin encoding)\n",
    "- read the file line-by-line\n",
    "- pre-process each line (tokenize text into individual words, remove punctuation, set to lowercase, etc)\n",
    "\n",
    "The file we're reading is a **corpus**.\n",
    "Each line of the file is a **document**.\n",
    "\n",
    ".. Important::\n",
    "  To train the model, we'll need to associate a tag/number with each document\n",
    "  of the training corpus. In our case, the tag is simply the zero-based line\n",
    "  number.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import smart_open\n",
    "\n",
    "def read_corpus(fname, tokens_only=False):\n",
    "    with smart_open.open(fname, encoding=\"iso-8859-1\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            tokens = gensim.utils.simple_preprocess(line)\n",
    "            if tokens_only:\n",
    "                yield tokens\n",
    "            else:\n",
    "                # For training data, add tags\n",
    "                yield gensim.models.doc2vec.TaggedDocument(tokens, [i])\n",
    "\n",
    "train_corpus = list(read_corpus(lee_train_file))\n",
    "test_corpus = list(read_corpus(lee_test_file, tokens_only=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the training corpus\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TaggedDocument(words=['hundreds', 'of', 'people', 'have', 'been', 'forced', 'to', 'vacate', 'their', 'homes', 'in', 'the', 'southern', 'highlands', 'of', 'new', 'south', 'wales', 'as', 'strong', 'winds', 'today', 'pushed', 'huge', 'bushfire', 'towards', 'the', 'town', 'of', 'hill', 'top', 'new', 'blaze', 'near', 'goulburn', 'south', 'west', 'of', 'sydney', 'has', 'forced', 'the', 'closure', 'of', 'the', 'hume', 'highway', 'at', 'about', 'pm', 'aedt', 'marked', 'deterioration', 'in', 'the', 'weather', 'as', 'storm', 'cell', 'moved', 'east', 'across', 'the', 'blue', 'mountains', 'forced', 'authorities', 'to', 'make', 'decision', 'to', 'evacuate', 'people', 'from', 'homes', 'in', 'outlying', 'streets', 'at', 'hill', 'top', 'in', 'the', 'new', 'south', 'wales', 'southern', 'highlands', 'an', 'estimated', 'residents', 'have', 'left', 'their', 'homes', 'for', 'nearby', 'mittagong', 'the', 'new', 'south', 'wales', 'rural', 'fire', 'service', 'says', 'the', 'weather', 'conditions', 'which', 'caused', 'the', 'fire', 'to', 'burn', 'in', 'finger', 'formation', 'have', 'now', 'eased', 'and', 'about', 'fire', 'units', 'in', 'and', 'around', 'hill', 'top', 'are', 'optimistic', 'of', 'defending', 'all', 'properties', 'as', 'more', 'than', 'blazes', 'burn', 'on', 'new', 'year', 'eve', 'in', 'new', 'south', 'wales', 'fire', 'crews', 'have', 'been', 'called', 'to', 'new', 'fire', 'at', 'gunning', 'south', 'of', 'goulburn', 'while', 'few', 'details', 'are', 'available', 'at', 'this', 'stage', 'fire', 'authorities', 'says', 'it', 'has', 'closed', 'the', 'hume', 'highway', 'in', 'both', 'directions', 'meanwhile', 'new', 'fire', 'in', 'sydney', 'west', 'is', 'no', 'longer', 'threatening', 'properties', 'in', 'the', 'cranebrook', 'area', 'rain', 'has', 'fallen', 'in', 'some', 'parts', 'of', 'the', 'illawarra', 'sydney', 'the', 'hunter', 'valley', 'and', 'the', 'north', 'coast', 'but', 'the', 'bureau', 'of', 'meteorology', 'claire', 'richards', 'says', 'the', 'rain', 'has', 'done', 'little', 'to', 'ease', 'any', 'of', 'the', 'hundred', 'fires', 'still', 'burning', 'across', 'the', 'state', 'the', 'falls', 'have', 'been', 'quite', 'isolated', 'in', 'those', 'areas', 'and', 'generally', 'the', 'falls', 'have', 'been', 'less', 'than', 'about', 'five', 'millimetres', 'she', 'said', 'in', 'some', 'places', 'really', 'not', 'significant', 'at', 'all', 'less', 'than', 'millimetre', 'so', 'there', 'hasn', 'been', 'much', 'relief', 'as', 'far', 'as', 'rain', 'is', 'concerned', 'in', 'fact', 'they', 've', 'probably', 'hampered', 'the', 'efforts', 'of', 'the', 'firefighters', 'more', 'because', 'of', 'the', 'wind', 'gusts', 'that', 'are', 'associated', 'with', 'those', 'thunderstorms'], tags=[0]), TaggedDocument(words=['indian', 'security', 'forces', 'have', 'shot', 'dead', 'eight', 'suspected', 'militants', 'in', 'night', 'long', 'encounter', 'in', 'southern', 'kashmir', 'the', 'shootout', 'took', 'place', 'at', 'dora', 'village', 'some', 'kilometers', 'south', 'of', 'the', 'kashmiri', 'summer', 'capital', 'srinagar', 'the', 'deaths', 'came', 'as', 'pakistani', 'police', 'arrested', 'more', 'than', 'two', 'dozen', 'militants', 'from', 'extremist', 'groups', 'accused', 'of', 'staging', 'an', 'attack', 'on', 'india', 'parliament', 'india', 'has', 'accused', 'pakistan', 'based', 'lashkar', 'taiba', 'and', 'jaish', 'mohammad', 'of', 'carrying', 'out', 'the', 'attack', 'on', 'december', 'at', 'the', 'behest', 'of', 'pakistani', 'military', 'intelligence', 'military', 'tensions', 'have', 'soared', 'since', 'the', 'raid', 'with', 'both', 'sides', 'massing', 'troops', 'along', 'their', 'border', 'and', 'trading', 'tit', 'for', 'tat', 'diplomatic', 'sanctions', 'yesterday', 'pakistan', 'announced', 'it', 'had', 'arrested', 'lashkar', 'taiba', 'chief', 'hafiz', 'mohammed', 'saeed', 'police', 'in', 'karachi', 'say', 'it', 'is', 'likely', 'more', 'raids', 'will', 'be', 'launched', 'against', 'the', 'two', 'groups', 'as', 'well', 'as', 'other', 'militant', 'organisations', 'accused', 'of', 'targetting', 'india', 'military', 'tensions', 'between', 'india', 'and', 'pakistan', 'have', 'escalated', 'to', 'level', 'not', 'seen', 'since', 'their', 'war'], tags=[1])]\n"
     ]
    }
   ],
   "source": [
    "print(train_corpus[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the testing corpus looks like this:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['the', 'national', 'executive', 'of', 'the', 'strife', 'torn', 'democrats', 'last', 'night', 'appointed', 'little', 'known', 'west', 'australian', 'senator', 'brian', 'greig', 'as', 'interim', 'leader', 'shock', 'move', 'likely', 'to', 'provoke', 'further', 'conflict', 'between', 'the', 'party', 'senators', 'and', 'its', 'organisation', 'in', 'move', 'to', 'reassert', 'control', 'over', 'the', 'party', 'seven', 'senators', 'the', 'national', 'executive', 'last', 'night', 'rejected', 'aden', 'ridgeway', 'bid', 'to', 'become', 'interim', 'leader', 'in', 'favour', 'of', 'senator', 'greig', 'supporter', 'of', 'deposed', 'leader', 'natasha', 'stott', 'despoja', 'and', 'an', 'outspoken', 'gay', 'rights', 'activist'], ['cash', 'strapped', 'financial', 'services', 'group', 'amp', 'has', 'shelved', 'million', 'plan', 'to', 'buy', 'shares', 'back', 'from', 'investors', 'and', 'will', 'raise', 'million', 'in', 'fresh', 'capital', 'after', 'profits', 'crashed', 'in', 'the', 'six', 'months', 'to', 'june', 'chief', 'executive', 'paul', 'batchelor', 'said', 'the', 'result', 'was', 'solid', 'in', 'what', 'he', 'described', 'as', 'the', 'worst', 'conditions', 'for', 'stock', 'markets', 'in', 'years', 'amp', 'half', 'year', 'profit', 'sank', 'per', 'cent', 'to', 'million', 'or', 'share', 'as', 'australia', 'largest', 'investor', 'and', 'fund', 'manager', 'failed', 'to', 'hit', 'projected', 'per', 'cent', 'earnings', 'growth', 'targets', 'and', 'was', 'battered', 'by', 'falling', 'returns', 'on', 'share', 'markets']]\n"
     ]
    }
   ],
   "source": [
    "print(test_corpus[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the testing corpus is just a list of lists and does not contain\n",
    "any tags.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "Now, we'll instantiate a Doc2Vec model with a vector size with 50 dimensions and\n",
    "iterating over the training corpus 40 times. We set the minimum word count to\n",
    "2 in order to discard words with very few occurrences. (Without a variety of\n",
    "representative examples, retaining such infrequent words can often make a\n",
    "model worse!) Typical iteration counts in the published [Paragraph Vector paper](https://cs.stanford.edu/~quocle/paragraph_vector.pdf)_\n",
    "results, using 10s-of-thousands to millions of docs, are 10-20. More\n",
    "iterations take more time and eventually reach a point of diminishing\n",
    "returns.\n",
    "\n",
    "However, this is a very very small dataset (300 documents) with shortish\n",
    "documents (a few hundred words). Adding training passes can sometimes help\n",
    "with such small datasets.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-03 16:17:28,008 : INFO : Doc2Vec lifecycle event {'params': 'Doc2Vec(dm/m,d50,n5,w5,mc2,s0.001,t3)', 'datetime': '2023-02-03T16:17:28.008615', 'gensim': '4.1.2', 'python': '3.8.3 (default, Jul  2 2020, 17:30:36) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "model = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a vocabulary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-03 16:17:28,023 : INFO : collecting all words and their counts\n",
      "2023-02-03 16:17:28,024 : INFO : PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n",
      "2023-02-03 16:17:28,035 : INFO : collected 6981 word types and 300 unique tags from a corpus of 300 examples and 58152 words\n",
      "2023-02-03 16:17:28,035 : INFO : Creating a fresh vocabulary\n",
      "2023-02-03 16:17:28,050 : INFO : Doc2Vec lifecycle event {'msg': 'effective_min_count=2 retains 3955 unique words (56.653774530869505%% of original 6981, drops 3026)', 'datetime': '2023-02-03T16:17:28.050615', 'gensim': '4.1.2', 'python': '3.8.3 (default, Jul  2 2020, 17:30:36) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'prepare_vocab'}\n",
      "2023-02-03 16:17:28,051 : INFO : Doc2Vec lifecycle event {'msg': 'effective_min_count=2 leaves 55126 word corpus (94.79639565277205%% of original 58152, drops 3026)', 'datetime': '2023-02-03T16:17:28.051615', 'gensim': '4.1.2', 'python': '3.8.3 (default, Jul  2 2020, 17:30:36) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'prepare_vocab'}\n",
      "2023-02-03 16:17:28,072 : INFO : deleting the raw counts dictionary of 6981 items\n",
      "2023-02-03 16:17:28,073 : INFO : sample=0.001 downsamples 46 most-common words\n",
      "2023-02-03 16:17:28,073 : INFO : Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 42390.98914085061 word corpus (76.9%% of prior 55126)', 'datetime': '2023-02-03T16:17:28.073617', 'gensim': '4.1.2', 'python': '3.8.3 (default, Jul  2 2020, 17:30:36) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'prepare_vocab'}\n",
      "2023-02-03 16:17:28,104 : INFO : estimated required memory for 3955 words and 50 dimensions: 3679500 bytes\n",
      "2023-02-03 16:17:28,104 : INFO : resetting layer weights\n"
     ]
    }
   ],
   "source": [
    "model.build_vocab(train_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essentially, the vocabulary is a list (accessible via\n",
    "``model.wv.index_to_key``) of all of the unique words extracted from the training corpus.\n",
    "Additional attributes for each word are available using the ``model.wv.get_vecattr()`` method,\n",
    "For example, to see how many times ``penalty`` appeared in the training corpus:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 'penalty' appeared 4 times in the training corpus.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Word 'penalty' appeared {model.wv.get_vecattr('penalty', 'count')} times in the training corpus.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, train the model on the corpus.\n",
    "In the usual case, where Gensim installation found a BLAS library for optimized\n",
    "bulk vector operations, this training on this tiny 300 document, ~60k word corpus \n",
    "should take just a few seconds. (More realistic datasets of tens-of-millions\n",
    "of words or more take proportionately longer.) If for some reason a BLAS library \n",
    "isn't available, training uses a fallback approach that takes 60x-120x longer, \n",
    "so even this tiny training will take minutes rather than seconds. (And, in that \n",
    "case, you should also notice a warning in the logging letting you know there's \n",
    "something worth fixing.) So, be sure your installation uses the BLAS-optimized \n",
    "Gensim if you value your time.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-03 16:17:28,134 : INFO : Doc2Vec lifecycle event {'msg': 'training model with 3 workers on 3955 vocabulary and 50 features, using sg=0 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2023-02-03T16:17:28.134619', 'gensim': '4.1.2', 'python': '3.8.3 (default, Jul  2 2020, 17:30:36) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'train'}\n",
      "2023-02-03 16:17:28,167 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2023-02-03 16:17:28,168 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2023-02-03 16:17:28,169 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2023-02-03 16:17:28,170 : INFO : EPOCH - 1 : training on 58152 raw words (42579 effective words) took 0.0s, 1266750 effective words/s\n",
      "2023-02-03 16:17:28,201 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2023-02-03 16:17:28,203 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2023-02-03 16:17:28,204 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2023-02-03 16:17:28,204 : INFO : EPOCH - 2 : training on 58152 raw words (42575 effective words) took 0.0s, 1315017 effective words/s\n",
      "2023-02-03 16:17:28,237 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2023-02-03 16:17:28,239 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2023-02-03 16:17:28,240 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2023-02-03 16:17:28,240 : INFO : EPOCH - 3 : training on 58152 raw words (42653 effective words) took 0.0s, 1235323 effective words/s\n",
      "2023-02-03 16:17:28,271 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2023-02-03 16:17:28,273 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2023-02-03 16:17:28,275 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2023-02-03 16:17:28,275 : INFO : EPOCH - 4 : training on 58152 raw words (42762 effective words) took 0.0s, 1296176 effective words/s\n",
      "2023-02-03 16:17:28,309 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2023-02-03 16:17:28,310 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2023-02-03 16:17:28,311 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2023-02-03 16:17:28,311 : INFO : EPOCH - 5 : training on 58152 raw words (42707 effective words) took 0.0s, 1279948 effective words/s\n",
      "2023-02-03 16:17:28,343 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2023-02-03 16:17:28,346 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2023-02-03 16:17:28,347 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2023-02-03 16:17:28,347 : INFO : EPOCH - 6 : training on 58152 raw words (42652 effective words) took 0.0s, 1256370 effective words/s\n",
      "2023-02-03 16:17:28,377 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2023-02-03 16:17:28,378 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2023-02-03 16:17:28,379 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2023-02-03 16:17:28,379 : INFO : EPOCH - 7 : training on 58152 raw words (42718 effective words) took 0.0s, 1439863 effective words/s\n",
      "2023-02-03 16:17:28,411 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2023-02-03 16:17:28,412 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2023-02-03 16:17:28,414 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2023-02-03 16:17:28,415 : INFO : EPOCH - 8 : training on 58152 raw words (42675 effective words) took 0.0s, 1272729 effective words/s\n",
      "2023-02-03 16:17:28,447 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2023-02-03 16:17:28,448 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2023-02-03 16:17:28,451 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2023-02-03 16:17:28,451 : INFO : EPOCH - 9 : training on 58152 raw words (42779 effective words) took 0.0s, 1271482 effective words/s\n",
      "2023-02-03 16:17:28,485 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2023-02-03 16:17:28,486 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2023-02-03 16:17:28,487 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2023-02-03 16:17:28,488 : INFO : EPOCH - 10 : training on 58152 raw words (42661 effective words) took 0.0s, 1233658 effective words/s\n",
      "2023-02-03 16:17:28,519 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2023-02-03 16:17:28,520 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2023-02-03 16:17:28,522 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2023-02-03 16:17:28,523 : INFO : EPOCH - 11 : training on 58152 raw words (42640 effective words) took 0.0s, 1284136 effective words/s\n",
      "2023-02-03 16:17:28,555 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2023-02-03 16:17:28,557 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2023-02-03 16:17:28,557 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2023-02-03 16:17:28,557 : INFO : EPOCH - 12 : training on 58152 raw words (42750 effective words) took 0.0s, 1291104 effective words/s\n",
      "2023-02-03 16:17:28,592 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2023-02-03 16:17:28,593 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2023-02-03 16:17:28,594 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2023-02-03 16:17:28,594 : INFO : EPOCH - 13 : training on 58152 raw words (42700 effective words) took 0.0s, 1241658 effective words/s\n",
      "2023-02-03 16:17:28,627 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2023-02-03 16:17:28,629 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2023-02-03 16:17:28,630 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2023-02-03 16:17:28,630 : INFO : EPOCH - 14 : training on 58152 raw words (42761 effective words) took 0.0s, 1267056 effective words/s\n",
      "2023-02-03 16:17:28,665 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2023-02-03 16:17:28,667 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2023-02-03 16:17:28,669 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2023-02-03 16:17:28,669 : INFO : EPOCH - 15 : training on 58152 raw words (42761 effective words) took 0.0s, 1181122 effective words/s\n",
      "2023-02-03 16:17:28,702 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2023-02-03 16:17:28,703 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2023-02-03 16:17:28,705 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2023-02-03 16:17:28,705 : INFO : EPOCH - 16 : training on 58152 raw words (42851 effective words) took 0.0s, 1262473 effective words/s\n",
      "2023-02-03 16:17:28,740 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2023-02-03 16:17:28,741 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2023-02-03 16:17:28,743 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2023-02-03 16:17:28,743 : INFO : EPOCH - 17 : training on 58152 raw words (42648 effective words) took 0.0s, 1192641 effective words/s\n",
      "2023-02-03 16:17:28,777 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2023-02-03 16:17:28,780 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2023-02-03 16:17:28,781 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2023-02-03 16:17:28,781 : INFO : EPOCH - 18 : training on 58152 raw words (42740 effective words) took 0.0s, 1171758 effective words/s\n",
      "2023-02-03 16:17:28,814 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2023-02-03 16:17:28,816 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2023-02-03 16:17:28,817 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2023-02-03 16:17:28,818 : INFO : EPOCH - 19 : training on 58152 raw words (42768 effective words) took 0.0s, 1259961 effective words/s\n",
      "2023-02-03 16:17:28,849 : INFO : worker thread finished; awaiting finish of 2 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-03 16:17:28,851 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2023-02-03 16:17:28,853 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2023-02-03 16:17:28,853 : INFO : EPOCH - 20 : training on 58152 raw words (42712 effective words) took 0.0s, 1280040 effective words/s\n",
      "2023-02-03 16:17:28,886 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2023-02-03 16:17:28,887 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2023-02-03 16:17:28,888 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2023-02-03 16:17:28,889 : INFO : EPOCH - 21 : training on 58152 raw words (42664 effective words) took 0.0s, 1257980 effective words/s\n",
      "2023-02-03 16:17:28,921 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2023-02-03 16:17:28,923 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2023-02-03 16:17:28,923 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2023-02-03 16:17:28,923 : INFO : EPOCH - 22 : training on 58152 raw words (42684 effective words) took 0.0s, 1304153 effective words/s\n",
      "2023-02-03 16:17:28,954 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2023-02-03 16:17:28,956 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2023-02-03 16:17:28,957 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2023-02-03 16:17:28,957 : INFO : EPOCH - 23 : training on 58152 raw words (42638 effective words) took 0.0s, 1336187 effective words/s\n",
      "2023-02-03 16:17:28,990 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2023-02-03 16:17:28,991 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2023-02-03 16:17:28,993 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2023-02-03 16:17:28,993 : INFO : EPOCH - 24 : training on 58152 raw words (42620 effective words) took 0.0s, 1259795 effective words/s\n",
      "2023-02-03 16:17:29,024 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2023-02-03 16:17:29,025 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2023-02-03 16:17:29,026 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2023-02-03 16:17:29,027 : INFO : EPOCH - 25 : training on 58152 raw words (42663 effective words) took 0.0s, 1335748 effective words/s\n",
      "2023-02-03 16:17:29,059 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2023-02-03 16:17:29,061 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2023-02-03 16:17:29,063 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2023-02-03 16:17:29,063 : INFO : EPOCH - 26 : training on 58152 raw words (42618 effective words) took 0.0s, 1237464 effective words/s\n",
      "2023-02-03 16:17:29,094 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2023-02-03 16:17:29,095 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2023-02-03 16:17:29,097 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2023-02-03 16:17:29,097 : INFO : EPOCH - 27 : training on 58152 raw words (42716 effective words) took 0.0s, 1337956 effective words/s\n",
      "2023-02-03 16:17:29,128 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2023-02-03 16:17:29,130 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2023-02-03 16:17:29,131 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2023-02-03 16:17:29,131 : INFO : EPOCH - 28 : training on 58152 raw words (42698 effective words) took 0.0s, 1351459 effective words/s\n",
      "2023-02-03 16:17:29,164 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2023-02-03 16:17:29,165 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2023-02-03 16:17:29,165 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2023-02-03 16:17:29,166 : INFO : EPOCH - 29 : training on 58152 raw words (42763 effective words) took 0.0s, 1298079 effective words/s\n",
      "2023-02-03 16:17:29,197 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2023-02-03 16:17:29,199 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2023-02-03 16:17:29,201 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2023-02-03 16:17:29,201 : INFO : EPOCH - 30 : training on 58152 raw words (42652 effective words) took 0.0s, 1274225 effective words/s\n",
      "2023-02-03 16:17:29,233 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2023-02-03 16:17:29,234 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2023-02-03 16:17:29,236 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2023-02-03 16:17:29,237 : INFO : EPOCH - 31 : training on 58152 raw words (42662 effective words) took 0.0s, 1263898 effective words/s\n",
      "2023-02-03 16:17:29,268 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2023-02-03 16:17:29,271 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2023-02-03 16:17:29,271 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2023-02-03 16:17:29,272 : INFO : EPOCH - 32 : training on 58152 raw words (42705 effective words) took 0.0s, 1283878 effective words/s\n",
      "2023-02-03 16:17:29,305 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2023-02-03 16:17:29,306 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2023-02-03 16:17:29,308 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2023-02-03 16:17:29,308 : INFO : EPOCH - 33 : training on 58152 raw words (42663 effective words) took 0.0s, 1254407 effective words/s\n",
      "2023-02-03 16:17:29,338 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2023-02-03 16:17:29,339 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2023-02-03 16:17:29,339 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2023-02-03 16:17:29,339 : INFO : EPOCH - 34 : training on 58152 raw words (42709 effective words) took 0.0s, 1454488 effective words/s\n",
      "2023-02-03 16:17:29,371 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2023-02-03 16:17:29,373 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2023-02-03 16:17:29,374 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2023-02-03 16:17:29,374 : INFO : EPOCH - 35 : training on 58152 raw words (42695 effective words) took 0.0s, 1310306 effective words/s\n",
      "2023-02-03 16:17:29,404 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2023-02-03 16:17:29,406 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2023-02-03 16:17:29,407 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2023-02-03 16:17:29,408 : INFO : EPOCH - 36 : training on 58152 raw words (42689 effective words) took 0.0s, 1325190 effective words/s\n",
      "2023-02-03 16:17:29,441 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2023-02-03 16:17:29,442 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2023-02-03 16:17:29,444 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2023-02-03 16:17:29,444 : INFO : EPOCH - 37 : training on 58152 raw words (42647 effective words) took 0.0s, 1241272 effective words/s\n",
      "2023-02-03 16:17:29,476 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2023-02-03 16:17:29,479 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2023-02-03 16:17:29,480 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2023-02-03 16:17:29,480 : INFO : EPOCH - 38 : training on 58152 raw words (42745 effective words) took 0.0s, 1281175 effective words/s\n",
      "2023-02-03 16:17:29,514 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2023-02-03 16:17:29,516 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2023-02-03 16:17:29,516 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2023-02-03 16:17:29,517 : INFO : EPOCH - 39 : training on 58152 raw words (42667 effective words) took 0.0s, 1236560 effective words/s\n",
      "2023-02-03 16:17:29,548 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2023-02-03 16:17:29,552 : INFO : worker thread finished; awaiting finish of 1 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-03 16:17:29,552 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2023-02-03 16:17:29,553 : INFO : EPOCH - 40 : training on 58152 raw words (42770 effective words) took 0.0s, 1275673 effective words/s\n",
      "2023-02-03 16:17:29,553 : INFO : Doc2Vec lifecycle event {'msg': 'training on 2326080 raw words (1707760 effective words) took 1.4s, 1203508 effective words/s', 'datetime': '2023-02-03T16:17:29.553615', 'gensim': '4.1.2', 'python': '3.8.3 (default, Jul  2 2020, 17:30:36) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'train'}\n"
     ]
    }
   ],
   "source": [
    "model.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can use the trained model to infer a vector for any piece of text\n",
    "by passing a list of words to the ``model.infer_vector`` function. This\n",
    "vector can then be compared with other vectors via cosine similarity.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.25806528 -0.28324774 -0.11395606  0.23773898  0.07359454 -0.16515383\n",
      "  0.08651788 -0.02348572 -0.25206837 -0.21480533  0.11116183 -0.10674212\n",
      "  0.00243357 -0.07734905 -0.13793291 -0.07086124  0.07548655  0.21264476\n",
      "  0.23112404 -0.09301944  0.04770815  0.03114991  0.20847404  0.05721074\n",
      " -0.07945006 -0.07521646 -0.2631462  -0.02829382 -0.06889746 -0.04866637\n",
      "  0.32616365 -0.09635716  0.26221782  0.13091801  0.22140692  0.14503257\n",
      " -0.01704509 -0.26430637 -0.11462945  0.14668524 -0.02950232 -0.00913692\n",
      "  0.05246502 -0.10036547  0.05987159  0.08598381 -0.1682845  -0.10590987\n",
      "  0.0730333  -0.05360325]\n"
     ]
    }
   ],
   "source": [
    "vector = model.infer_vector(['only', 'you', 'can', 'prevent', 'forest', 'fires'])\n",
    "print(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that ``infer_vector()`` does *not* take a string, but rather a list of\n",
    "string tokens, which should have already been tokenized the same way as the\n",
    "``words`` property of original training document objects.\n",
    "\n",
    "Also note that because the underlying training/inference algorithms are an\n",
    "iterative approximation problem that makes use of internal randomization,\n",
    "repeated inferences of the same text will return slightly different vectors.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessing the Model\n",
    "\n",
    "To assess our new model, we'll first infer new vectors for each document of\n",
    "the training corpus, compare the inferred vectors with the training corpus,\n",
    "and then returning the rank of the document based on self-similarity.\n",
    "Basically, we're pretending as if the training corpus is some new unseen data\n",
    "and then seeing how they compare with the trained model. The expectation is\n",
    "that we've likely overfit our model (i.e., all of the ranks will be less than\n",
    "2) and so we should be able to find similar documents very easily.\n",
    "Additionally, we'll keep track of the second ranks for a comparison of less\n",
    "similar documents.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks = []\n",
    "second_ranks = []\n",
    "for doc_id in range(len(train_corpus)):\n",
    "    inferred_vector = model.infer_vector(train_corpus[doc_id].words)\n",
    "    sims = model.dv.most_similar([inferred_vector], topn=len(model.dv))\n",
    "    rank = [docid for docid, sim in sims].index(doc_id)\n",
    "    ranks.append(rank)\n",
    "\n",
    "    second_ranks.append(sims[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's count how each document ranks with respect to the training corpus\n",
    "\n",
    "NB. Results vary between runs due to random seeding and very small corpus\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 291, 1: 9})\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "counter = collections.Counter(ranks)\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically, greater than 95% of the inferred documents are found to be most\n",
    "similar to itself and about 5% of the time it is mistakenly most similar to\n",
    "another document. Checking the inferred-vector against a\n",
    "training-vector is a sort of 'sanity check' as to whether the model is\n",
    "behaving in a usefully consistent manner, though not a real 'accuracy' value.\n",
    "\n",
    "This is great and not entirely surprising. We can take a look at an example:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document (299): Â«australia will take on france in the doubles rubber of the davis cup tennis final today with the tie levelled at wayne arthurs and todd woodbridge are scheduled to lead australia in the doubles against cedric pioline and fabrice santoro however changes can be made to the line up up to an hour before the match and australian team captain john fitzgerald suggested he might do just that we ll make team appraisal of the whole situation go over the pros and cons and make decision french team captain guy forget says he will not make changes but does not know what to expect from australia todd is the best doubles player in the world right now so expect him to play he said would probably use wayne arthurs but don know what to expect really pat rafter salvaged australia davis cup campaign yesterday with win in the second singles match rafter overcame an arm injury to defeat french number one sebastien grosjean in three sets the australian says he is happy with his form it not very pretty tennis there isn too many consistent bounces you are playing like said bit of classic old grass court rafter said rafter levelled the score after lleyton hewitt shock five set loss to nicholas escude in the first singles rubber but rafter says he felt no added pressure after hewitt defeat knew had good team to back me up even if we were down he said knew could win on the last day know the boys can win doubles so even if we were down still feel we are good enough team to win and vice versa they are good enough team to beat us as wellÂ»\n",
      "\n",
      "SIMILAR/DISSIMILAR DOCS PER MODEL Doc2Vec(dm/m,d50,n5,w5,mc2,s0.001,t3):\n",
      "\n",
      "MOST (299, 0.947127640247345): Â«australia will take on france in the doubles rubber of the davis cup tennis final today with the tie levelled at wayne arthurs and todd woodbridge are scheduled to lead australia in the doubles against cedric pioline and fabrice santoro however changes can be made to the line up up to an hour before the match and australian team captain john fitzgerald suggested he might do just that we ll make team appraisal of the whole situation go over the pros and cons and make decision french team captain guy forget says he will not make changes but does not know what to expect from australia todd is the best doubles player in the world right now so expect him to play he said would probably use wayne arthurs but don know what to expect really pat rafter salvaged australia davis cup campaign yesterday with win in the second singles match rafter overcame an arm injury to defeat french number one sebastien grosjean in three sets the australian says he is happy with his form it not very pretty tennis there isn too many consistent bounces you are playing like said bit of classic old grass court rafter said rafter levelled the score after lleyton hewitt shock five set loss to nicholas escude in the first singles rubber but rafter says he felt no added pressure after hewitt defeat knew had good team to back me up even if we were down he said knew could win on the last day know the boys can win doubles so even if we were down still feel we are good enough team to win and vice versa they are good enough team to beat us as wellÂ»\n",
      "\n",
      "SECOND-MOST (104, 0.795806348323822): Â«australian cricket captain steve waugh has supported fast bowler brett lee after criticism of his intimidatory bowling to the south african tailenders in the first test in adelaide earlier this month lee was fined for giving new zealand tailender shane bond an unsportsmanlike send off during the third test in perth waugh says tailenders should not be protected from short pitched bowling these days you re earning big money you ve got responsibility to learn how to bat he said mean there no times like years ago when it was not professional and sort of bowlers code these days you re professional our batsmen work very hard at their batting and expect other tailenders to do likewise meanwhile waugh says his side will need to guard against complacency after convincingly winning the first test by runs waugh says despite the dominance of his side in the first test south africa can never be taken lightly it only one test match out of three or six whichever way you want to look at it so there lot of work to go he said but it nice to win the first battle definitely it gives us lot of confidence going into melbourne you know the big crowd there we love playing in front of the boxing day crowd so that will be to our advantage as well south africa begins four day match against new south wales in sydney on thursday in the lead up to the boxing day test veteran fast bowler allan donald will play in the warm up match and is likely to take his place in the team for the second test south african captain shaun pollock expects much better performance from his side in the melbourne test we still believe that we didn play to our full potential so if we can improve on our aspects the output we put out on the field will be lot better and we still believe we have side that is good enough to beat australia on our day he saidÂ»\n",
      "\n",
      "MEDIAN (238, 0.24940620362758636): Â«centrelink is urging people affected by job cuts at regional pay tv operator austar and travel company traveland to seek information about their income support options traveland has announced it is shedding more than jobs around australia and austar is letting employees go centrelink finance information officer peter murray says those facing uncertain futures should head to centrelink in the next few days centrelink is the shopfront now for commonwealth services for income support and the employment network so that it is important if people haven been to us before they might get pleasant surprise at the range of services that we do offer to try and help them through situations where things might have changed for them mr murray saidÂ»\n",
      "\n",
      "LEAST (243, -0.11174564063549042): Â«four afghan factions have reached agreement on an interim cabinet during talks in germany the united nations says the administration which will take over from december will be headed by the royalist anti taliban commander hamed karzai it concludes more than week of negotiations outside bonn and is aimed at restoring peace and stability to the war ravaged country the year old former deputy foreign minister who is currently battling the taliban around the southern city of kandahar is an ally of the exiled afghan king mohammed zahir shah he will serve as chairman of an interim authority that will govern afghanistan for six month period before loya jirga or grand traditional assembly of elders in turn appoints an month transitional government meanwhile united states marines are now reported to have been deployed in eastern afghanistan where opposition forces are closing in on al qaeda soldiers reports from the area say there has been gun battle between the opposition and al qaeda close to the tora bora cave complex where osama bin laden is thought to be hiding in the south of the country american marines are taking part in patrols around the air base they have secured near kandahar but are unlikely to take part in any assault on the city however the chairman of the joint chiefs of staff general richard myers says they are prepared for anything they are prepared for engagements they re robust fighting force and they re absolutely ready to engage if that required he saidÂ»\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Document ({}): Â«{}Â»\\n'.format(doc_id, ' '.join(train_corpus[doc_id].words)))\n",
    "print(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\\n' % model)\n",
    "for label, index in [('MOST', 0), ('SECOND-MOST', 1), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:\n",
    "    print(u'%s %s: Â«%sÂ»\\n' % (label, sims[index], ' '.join(train_corpus[sims[index][0]].words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice above that the most similar document (usually the same text) is has a\n",
    "similarity score approaching 1.0. However, the similarity score for the\n",
    "second-ranked documents should be significantly lower (assuming the documents\n",
    "are in fact different) and the reasoning becomes obvious when we examine the\n",
    "text itself.\n",
    "\n",
    "We can run the next cell repeatedly to see a sampling other target-document\n",
    "comparisons.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Document (277): Â«israeli soldiers have shot dead five palestinians in two west bank towns an israeli military source said the soldiers shot four palestinians near jenin when palestinian gunmen opened fire on an army patrol and the troops returned fire another palestinian was killed by israeli soldiers near the west bank city of tulkarem palestinian security source said meanwhile palestinian police have arrested three senior leaders of the hardline hamas group in crackdown that netted more than islamic militants following wave of suicide attacks in israel palestinian security source told the afp news agency hamas official confirmed the arrests of two senior leaders ismail abu shanab and ismail haniya and said police have issued arrest warrants for another two but he refused to name them the security source said more than militants from hamas and the smaller islamic jihad were rounded after yasser arafat palestinian leadership vowed to crackdown on them for wave of anti israeli suicide assaults most of the arrests came after the palestinian leadership declared state of emergency in the palestinian territories giving police sweeping powers to round up militantsÂ»\n",
      "\n",
      "Similar Document (208, 0.9014468193054199): Â«israeli tanks and troops have launched two incursions in the gaza strip near the palestinian self rule city of khan yunis arresting several people and searching houses witnesses say undercover soldiers wearing masks arrived first followed by tanks and additional troops palestinian officials say they were looking for iffam abu daka one of the leaders of the militant democratic front earlier an israeli fighter jet struck three buildings in the palestinian police headquarters in gaza city injuring at least people palestinian officials say two four storey buildings inside the compound were engulfed in flames and destroyedÂ»\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Pick a random document from the corpus and infer a vector from the model\n",
    "import random\n",
    "doc_id = random.randint(0, len(train_corpus) - 1)\n",
    "\n",
    "# Compare and print the second-most-similar document\n",
    "print('Train Document ({}): Â«{}Â»\\n'.format(doc_id, ' '.join(train_corpus[doc_id].words)))\n",
    "sim_id = second_ranks[doc_id]\n",
    "print('Similar Document {}: Â«{}Â»\\n'.format(sim_id, ' '.join(train_corpus[sim_id[0]].words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Model\n",
    "\n",
    "Using the same approach above, we'll infer the vector for a randomly chosen\n",
    "test document, and compare the document to our model by eye.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Document (32): Â«at least three democrats are considering splitting from the party while no one has yet nominated to contest the leadership three of the gang of four senators who ousted natasha stott despoja from the leadership are considering forming new progressive centre party in the fallout from last week turmoil this would leave the democrats with rump of three or four members west australian senator andrew murray said yesterday unless the democrats left wing gave ground the party would splitÂ»\n",
      "\n",
      "SIMILAR/DISSIMILAR DOCS PER MODEL Doc2Vec(dm/m,d50,n5,w5,mc2,s0.001,t3):\n",
      "\n",
      "MOST (3, 0.4287218153476715): Â«argentina political and economic crisis has deepened with the resignation of its interim president who took office just week ago aldolfo rodregiuez saa told stunned nation that he could not rescue argentina because key fellow peronists would not support his default on massive foreign debt repayment or his plan for new currency it was only week ago that he was promising million new jobs to end four years of recession days after his predecessor resigned following series of failed rescue packages after announcing that the senate leader ramon puerta would assume the presidency until congress appoints new caretaker president the government said he too had quit and another senior lawmaker would act in the role fresh elections are not scheduled until march leaving whoever assumes the presidency with the daunting task of tackling argentina worst crisis in years but this time isolated by international lending agenciesÂ»\n",
      "\n",
      "MEDIAN (180, 0.20267054438591003): Â«australia has linked million of aid to new agreement with nauru to accept an extra asylum seekers the deal means nauru will take up to asylum seekers under australia pacific solution foreign minister alexander downer signed the understanding today with nauru president rene harris mr downer inspected the nauru camps and says they are are practical and efficient had good look at the sanitation the ablution blocks and thought they were pretty good he said the asylum seekers have various things to do there are volleyball facilities and soccer facilities television is available they can see different channels on tv the catering is good there are three meals day providedÂ»\n",
      "\n",
      "LEAST (260, -0.030364632606506348): Â«traveland wholly owned travel centres have ceased operating from today leaving more than staff seeking other jobs the failed company administrators say they have buyer for traveland franchise network but have not been able to save the company stores one of the administrators richard albarran says the deal which is yet to be approved by committee formed today of creditors will unfortunately leave hundreds of people who have booked holidays through the company stores out of pocket the dollar value is approximately tad over million mr albarran said they will now be entitled to make claims through the travel compensation fund the meeting of company creditors was told this morning staff are owed nearly million in entitlements the australian services union luke foley says he will be doing everything he can to ensure they receive every cent ansett administrators are liable for perhaps the lion share of those employee entitlements we re confident that ll be met mr foley said and more than staff are to lose their jobs at australia biggest regional pay television operator austar the company has this morning announced wide ranging restructuring plans management at the struggling pay tv operator has now completed review of all its activities as result of this review the austar board has decided to outsource number of existing functions cease operating its own internet network and streamline other processes the company anticipates annualised savings of around million more than staff will be made redundant from the end of december austar has given assurances that redundant workers will receive their full entitlements and redundancy payments in line with company policy the company says they will receive all statutory entitlements and redundancy payments in line with company policy on the stock exchange austar shares rose five cents to cents shortly before pm aedtÂ»\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Pick a random document from the test corpus and infer a vector from the model\n",
    "doc_id = random.randint(0, len(test_corpus) - 1)\n",
    "inferred_vector = model.infer_vector(test_corpus[doc_id])\n",
    "sims = model.dv.most_similar([inferred_vector], topn=len(model.dv))\n",
    "\n",
    "# Compare and print the most/median/least similar documents from the train corpus\n",
    "print('Test Document ({}): Â«{}Â»\\n'.format(doc_id, ' '.join(test_corpus[doc_id])))\n",
    "print(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\\n' % model)\n",
    "for label, index in [('MOST', 0), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:\n",
    "    print(u'%s %s: Â«%sÂ»\\n' % (label, sims[index], ' '.join(train_corpus[sims[index][0]].words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Let's review what we've seen in this tutorial:\n",
    "\n",
    "0. Review the relevant models: bag-of-words, Word2Vec, Doc2Vec\n",
    "1. Load and preprocess the training and test corpora (see `core_concepts_corpus`)\n",
    "2. Train a Doc2Vec `core_concepts_model` model using the training corpus\n",
    "3. Demonstrate how the trained model can be used to infer a `core_concepts_vector`\n",
    "4. Assess the model\n",
    "5. Test the model on the test corpus\n",
    "\n",
    "That's it! Doc2Vec is a great way to explore relationships between documents.\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "If you'd like to know more about the subject matter of this tutorial, check out the links below.\n",
    "\n",
    "* [Word2Vec Paper](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)\n",
    "* [Doc2Vec Paper](https://cs.stanford.edu/~quocle/paragraph_vector.pdf)\n",
    "* [Dr. Michael D. Lee's Website](http://faculty.sites.uci.edu/mdlee)\n",
    "* [Lee Corpus](http://faculty.sites.uci.edu/mdlee/similarity-data/)_\n",
    "* [IMDB Doc2Vec Tutorial](doc2vec-IMDB.ipynb)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
